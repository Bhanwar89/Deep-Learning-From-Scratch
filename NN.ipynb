{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simple Structure of a NN :-\n",
    "X or $a^{0}$ or Input Layer/Vector ----> Hidden Layer or $a^{1}$, $a^{2}$, ....., $a^{n}$ ----> OutPut Layer or $\\hat{y}$\n",
    "\n",
    "For each set of Hidden layer we have Neurons and Neurons are the part where all the math's happens\n",
    "\n",
    "Q. What Single Neuron Do in Forward Propagation?\n",
    "\n",
    "Input = $a^{l-1}$\n",
    "Output = $a^{l}$ Input Value Needed for the Next Layer\n",
    "\n",
    "$z^{l}$ = $w^{l}$ * $a^{l-1}$ + $b^{l}$\n",
    "\n",
    "$a^{l}$ = $g^{l}$($z^{l}$) :: Where $g^{l}$ is the Activation Function for that Neuron\n",
    "\n",
    "For Implementation We also Store the Values of :- $z^{l}$, $w^{l}$, $b^{l}$\n",
    "\n",
    "Q. For Backward Propagation we adjust the values of Weights $w^{l}$, $b^{l}$ ?\n",
    "\n",
    "We do  Gradient Descent on the Cost Function which is Sum of Loss Function \n",
    "\n",
    "Input = $da^{l}$ is the Partial Derivative of dw/da ((((???))))\n",
    "\n",
    "Output = $da^{l-1}$, $dw^{l}$, $db^{l}$\n",
    "\n",
    "$dz^{l}$ = $da^{l}$ * $g^{l}z^{l}$\n",
    "\n",
    "$dw^{l}$ = $dz^{l}$ * $\\mathbf{a^{l-1}}^\\top$\n",
    "\n",
    "$db^{l}$ = $dz^{l}$\n",
    "\n",
    "$da^{l-1}$ = $\\mathbf{w^{l}}^\\top$ * $dz^{l}$\n",
    "\n",
    "\n",
    "After Every Step of Backward Propagation The Values of $a^{1}$ Will be\n",
    "\n",
    "??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our NN will have a simple two-layer architecture. Input layer $a^{[0]}$ will have 784 units corresponding to the 784 pixels in each 28x28 input image. A hidden layer $a^{[1]}$ will have 10 units with ReLU activation, and finally our output layer $a^{[2]}$ will have 10 units corresponding to the ten digit classes with softmax activation.\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]}$$\n",
    "$$A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))$$\n",
    "$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$$\n",
    "$$A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y$$\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n",
    "$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$\n",
    "\n",
    "**Parameter updates**\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "Output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "\n",
    "print(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# Layers This is a layer of Where inputs are the values we got from the  Previses Layers and \n",
    "# So Baicely A^l and Weights and Bias are the values from each \n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# Cleaner Version of Layers\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "[0.5, -0.91, 0.26, -0.5],\n",
    "[-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = [] # Output of current layer\n",
    "for neuron_weights, neuron_bias in zip(weights, biases) : # Zip combine Two List Element Wise eg W = [1.5, 2.3, 3.5] & B = [1, 6, 5] Zip(W,B) will be [[1.5, 1], [2.3, 6], [3.5, 5]]\n",
    "    neuron_output = 0 # Output of given neuron\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs. append (neuron_output)\n",
    "\n",
    "print(layer_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "# Dot Product Single\n",
    "import numpy as np\n",
    "\n",
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "Output = np.dot(weights, inputs) + bias\n",
    "\n",
    "print(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "# Layer using Dot Product\n",
    "\n",
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "[0.5, -0.91, 0.26, -0.5],\n",
    "[-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "Output = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "source": [
    "# Why Need to Transpose Shape Did not Match for Weights and Input Matrixes\n",
    "# This Single Layer for Batches  Img For understanding should be in \n",
    "\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "[2.0, 5.0, -1.0, 2.0],\n",
    "[-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights =[[0.2, 0.8, - 0.5, 1.0],\n",
    "[0.5, -0.91, 0.26, -0.5],\n",
    "[-0.26, -0.27, 0.17, 0.87] ]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "output = np.dot (inputs, np.array(weights) .T) + biases\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[[1, 2, 3, 2.5],\n",
    "[2.0, 5.0, -1.0, 2.0],\n",
    "[-1.5, 2.7, 3.3, -0.8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def Layer_Dense(n_input, n_neuron):\n",
    "    Weights = 0.10 * np.random.randn(n_input, n_neuron)\n",
    "    Biases = np.zero((1, n_neuron))\n",
    "    return Weights, Biases\n",
    "\n",
    "def Forward(inputs,Weights,Biases ):\n",
    "    output = np.dot(inputs, Weights) + Biases\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11393755 -0.01423452]\n",
      " [ 0.04397295 -0.11042021]\n",
      " [ 0.11655307 -0.04529458]]\n"
     ]
    }
   ],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer1 = Layer_Dense(4,5)\n",
    "layer2 = Layer_Dense(5,2)\n",
    "\n",
    "layer1.forward(X)\n",
    "#print(layer1.output)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JLAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
